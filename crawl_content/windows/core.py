import re
import time
import base64
import requests
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from openpyxl import Workbook
from openpyxl.drawing.image import Image as ExcelImage
from openpyxl.styles import Alignment, Font
from io import BytesIO
from PIL import Image
import logging

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def setup_driver():
    """Kh·ªüi t·∫°o Chrome driver v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u"""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
    options.page_load_strategy = 'eager'  # Kh√¥ng ƒë·ª£i t·∫£i h·∫øt trang

    return webdriver.Chrome(options=options)


def download_image(url, save_path, timeout=15):
    """T·∫£i ·∫£nh t·ª´ URL v·ªÅ m√°y"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://diakov.net/'
        }

        logger.info(f"  üì• ƒêang t·∫£i ·∫£nh: {url[:60]}...")
        response = requests.get(url, headers=headers, timeout=timeout, stream=True)
        response.raise_for_status()

        # Ki·ªÉm tra content-type
        content_type = response.headers.get('content-type', '')
        if 'image' not in content_type:
            logger.warning(f"  ‚ö†Ô∏è Kh√¥ng ph·∫£i ·∫£nh: {content_type}")
            return False

        img = Image.open(BytesIO(response.content))

        # Convert sang RGB n·∫øu c·∫ßn
        if img.mode in ('RGBA', 'P'):
            img = img.convert('RGB')

        # Resize n·∫øu ·∫£nh qu√° l·ªõn
        if img.width > 800 or img.height > 800:
            img.thumbnail((800, 800), Image.Resampling.LANCZOS)

        img.save(save_path, 'JPEG', quality=85)
        logger.info(f"  ‚úì ƒê√£ l∆∞u: {save_path.name}")
        return True

    except Exception as e:
        logger.error(f"  ‚úó Kh√¥ng t·∫£i ƒë∆∞·ª£c ·∫£nh {url[:60]}: {str(e)}")
        return False


def extract_download_links(driver):
    """Tr√≠ch xu·∫•t c√°c link download t·ª´ div.zvcwers"""
    download_links = []

    try:
        # T√¨m div ch·ª©a link download
        download_div = driver.find_element(By.CSS_SELECTOR, "div.zvcwers")

        # L·∫•y t·∫•t c·∫£ c√°c th·∫ª <a> trong div
        links = download_div.find_elements(By.TAG_NAME, "a")

        for link in links:
            href = link.get_attribute("href")
            text = link.text.strip()

            # B·ªè qua link VIP ho·∫∑c link r·ªóng
            if not href or not text or "buy.php" in href or "VIP" in text:
                continue

            # L·∫•y t√™n host t·ª´ text ho·∫∑c t·ª´ URL
            if text:
                host_name = text
            else:
                # Parse domain t·ª´ URL
                match = re.search(r'https?://(?:www\.)?([^/]+)', href)
                host_name = match.group(1) if match else "Unknown"

            download_links.append({
                "host": host_name,
                "url": href
            })

        logger.info(f"  ‚Üí T√¨m th·∫•y {len(download_links)} link download")

    except Exception as e:
        logger.warning(f"  ‚Üí Kh√¥ng t√¨m th·∫•y link download: {str(e)}")

    return download_links


def extract_article_data(driver, url):
    """Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ m·ªôt b√†i vi·∫øt"""
    try:
        driver.get(url)
        wait = WebDriverWait(driver, 10)

        # ƒê·ª£i n·ªôi dung ch√≠nh load
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "h1.ftitle.card-title")))

        # 1. L·∫•y tag v√† title
        title_elem = driver.find_element(By.CSS_SELECTOR, "h1.ftitle.card-title")
        tag = title_elem.find_element(By.TAG_NAME, "a").text.strip()
        title = title_elem.find_element(By.TAG_NAME, "u").text.strip()

        # 2. L·∫•y m√¥ t·∫£
        body_elem = driver.find_element(By.CSS_SELECTOR, "div.card-body.sh")
        full_text = body_elem.text.strip()

        # Lo·∫°i b·ªè ph·∫ßn kh√¥ng c·∫ßn thi·∫øt
        patterns = [
            r'T√≠nh nƒÉng.*',
            r'ƒê·∫∑c ƒëi·ªÉm.*',
            r'HƒêH:.*',
            r'T·∫£i xu·ªëng.*',
            r't·ª´.*Github.*',
            r'ƒê√£ c·∫£m ∆°n:.*'
        ]

        description = full_text
        for pattern in patterns:
            description = re.split(pattern, description, flags=re.IGNORECASE)[0]

        # Gi·ªõi h·∫°n ƒë·ªô d√†i m√¥ t·∫£
        words = description.split()[:150]
        description = ' '.join(words).strip()

        # 3. L·∫•y ·∫£nh (t·ªëi ƒëa 3 ·∫£nh)
        # Scroll ƒë·ªÉ lazy load images
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1)

        imgs = driver.find_elements(By.CSS_SELECTOR, "div.card-body.sh img")
        image_urls = []

        logger.info(f"  ‚Üí T√¨m th·∫•y {len(imgs)} th·∫ª img")

        for idx, img in enumerate(imgs, start=1):
            # Th·ª≠ nhi·ªÅu attribute
            src = (img.get_attribute("src") or
                   img.get_attribute("data-src") or
                   img.get_attribute("data-lazy-src"))

            logger.debug(f"    ·∫¢nh {idx}: src={src}")

            # B·ªè qua ·∫£nh placeholder gif
            if not src or 'data:image/gif' in src or src.endswith('.gif'):
                continue

            # X·ª≠ l√Ω URL t∆∞∆°ng ƒë·ªëi
            if src.startswith("/"):
                src = "https://diakov.net" + src

            # Ch·ªâ l·∫•y ·∫£nh th·ª±c s·ª± (kh√¥ng ph·∫£i icon, spoiler, rating)
            if src.startswith("http"):
                # Ki·ªÉm tra k√≠ch th∆∞·ªõc ·∫£nh ƒë·ªÉ lo·∫°i b·ªè icon nh·ªè
                try:
                    width = img.get_attribute("width") or img.size.get('width', 0)
                    height = img.get_attribute("height") or img.size.get('height', 0)

                    # Ch·ªâ l·∫•y ·∫£nh c√≥ k√≠ch th∆∞·ªõc h·ª£p l√Ω
                    if width and height and (int(width) < 50 or int(height) < 50):
                        continue
                except:
                    pass

                # B·ªè qua icon, spoiler, rating
                if not any(x in src.lower() for x in ['icon', 'spoiler', 'rating', 'dleimages']):
                    image_urls.append(src)
                    logger.info(f"    ‚úì L·∫•y ·∫£nh: {src[:80]}...")

                    if len(image_urls) >= 3:
                        break

        # 4. L·∫•y link download
        download_links = extract_download_links(driver)

        logger.info(f"‚úì Crawled: {title}")

        return {
            "url": url,
            "tag": tag,
            "title": title,
            "description": description,
            "images": image_urls,
            "downloads": download_links
        }

    except Exception as e:
        logger.error(f"‚úó L·ªói crawl {url}: {str(e)}")
        return None


def save_to_excel(data, output_file="soft_data.xlsx"):
    """L∆∞u d·ªØ li·ªáu v√†o Excel v·ªõi format ƒë·∫πp"""
    wb = Workbook()
    ws = wb.active
    ws.title = "Software Data"

    # Header
    headers = ["STT", "Tag", "Title", "Description", "Image 1 URL", "Image 2 URL", "Image 3 URL", "Download Links",
               "URL"]
    ws.append(headers)

    # Style header
    for cell in ws[1]:
        cell.font = Font(bold=True, size=12)
        cell.alignment = Alignment(horizontal='center', vertical='center')

    # ƒêi·ªÅu ch·ªânh ƒë·ªô r·ªông c·ªôt
    ws.column_dimensions['A'].width = 5
    ws.column_dimensions['B'].width = 20
    ws.column_dimensions['C'].width = 30
    ws.column_dimensions['D'].width = 60
    ws.column_dimensions['E'].width = 60
    ws.column_dimensions['F'].width = 60
    ws.column_dimensions['G'].width = 60
    ws.column_dimensions['H'].width = 50
    ws.column_dimensions['I'].width = 40

    row_num = 2
    for idx, item in enumerate(data, start=1):
        if not item:
            continue

        # Th√™m d·ªØ li·ªáu text
        ws.cell(row=row_num, column=1, value=idx)
        ws.cell(row=row_num, column=2, value=item["tag"])
        ws.cell(row=row_num, column=3, value=item["title"])
        ws.cell(row=row_num, column=4, value=item["description"])

        # Th√™m URL ·∫£nh v√†o 3 c·ªôt ri√™ng bi·ªát
        images = item.get("images", [])
        for img_idx in range(3):
            col = 5 + img_idx  # C·ªôt E, F, G
            if img_idx < len(images):
                ws.cell(row=row_num, column=col, value=images[img_idx])
            else:
                ws.cell(row=row_num, column=col, value="")

        # Format download links
        download_text = ""
        for dl in item.get("downloads", []):
            download_text += f"{dl['host']}: {dl['url']}\n"
        ws.cell(row=row_num, column=8, value=download_text.strip())

        ws.cell(row=row_num, column=9, value=item["url"])

        # Wrap text cho description v√† download links
        ws.cell(row=row_num, column=4).alignment = Alignment(wrap_text=True, vertical='top')
        ws.cell(row=row_num, column=8).alignment = Alignment(wrap_text=True, vertical='top')

        # Wrap text cho image URLs
        for col in range(5, 8):
            ws.cell(row=row_num, column=col).alignment = Alignment(wrap_text=True, vertical='top')

        row_num += 1

    wb.save(output_file)
    logger.info(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o '{output_file}'")


def crawl_core(urls):
    """H√†m ch√≠nh ƒë·ªÉ crawl danh s√°ch URLs"""
    driver = setup_driver()
    data = []

    try:
        total = len(urls)
        for idx, url in enumerate(urls, start=1):
            logger.info(f"[{idx}/{total}] ƒêang crawl: {url}")

            article_data = extract_article_data(driver, url)
            if article_data:
                data.append(article_data)

            # Ngh·ªâ ng·∫Øn gi·ªØa c√°c request
            time.sleep(1)

    finally:
        driver.quit()

    return data

# Module n√†y export c√°c h√†m ƒë·ªÉ s·ª≠ d·ª•ng ·ªü n∆°i kh√°c:
# - setup_driver(): Kh·ªüi t·∫°o Chrome driver
# - extract_article_data(driver, url): Crawl m·ªôt b√†i vi·∫øt
# - crawl_core(urls): Crawl danh s√°ch URLs
# - save_to_excel(data, output_file): L∆∞u d·ªØ li·ªáu ra Excel